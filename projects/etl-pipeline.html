<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ETL Pipeline Project - Omar Portfolio</title>
    <meta name="description" content="Automated ETL Pipeline processing 1M+ records daily using Apache Airflow and PostgreSQL">
    <link rel="stylesheet" href="../css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-text">Omar Abdullatif</span>
                <span class="logo-subtitle">Data Engineer</span>
            </a>
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="../index.html" class="nav-link">Home</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#projects" class="nav-link">Projects</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#contact" class="nav-link">Contact</a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Project Hero -->
    <section class="project-hero">
        <div class="container">
            <div class="project-hero-content">
                <div class="project-breadcrumb">
                    <a href="../index.html">Home</a>
                    <span>/</span>
                    <a href="../index.html#projects">Projects</a>
                    <span>/</span>
                    <span>ETL Pipeline</span>
                </div>
                <h1 class="project-hero-title">Automated ETL Pipeline</h1>
                <p class="project-hero-subtitle">
                    Scalable data processing system handling 1M+ records daily with automated quality checks and error handling
                </p>
                <div class="project-hero-meta">
                    <div class="project-links">
                        <a href="https://github.com/omarabdullatiff/SQL-Data-Warehouse-Project" class="btn btn-primary">
                            <i class="fab fa-github"></i>
                            View Code
                        </a>
                        <a href="#demo" class="btn btn-secondary">
                            <i class="fas fa-play"></i>
                            Live Demo
                        </a>
                    </div>
                    <div class="project-tech-stack">
                        <span class="tech-tag">Python</span>
                        <span class="tech-tag">Apache Airflow</span>
                        <span class="tech-tag">PostgreSQL</span>
                        <span class="tech-tag">Docker</span>
                        <span class="tech-tag">Redis</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Project Overview -->
    <section class="project-section">
        <div class="container">
            <div class="project-grid">
                <div class="project-content">
                    <h2>Project Overview</h2>
                    <p>
                        This ETL pipeline was designed to solve the challenge of processing large volumes of heterogeneous data 
                        from multiple sources in a reliable, scalable manner. The system automatically extracts data from various 
                        APIs, transforms it according to business rules, and loads it into a data warehouse for analytics.
                    </p>
                    
                    <h3>Business Problem</h3>
                    <p>
                        The organization was struggling with manual data processing that took hours each day, was prone to errors, 
                        and couldn't scale with growing data volumes. They needed an automated solution that could:
                    </p>
                    <ul>
                        <li>Process data from 15+ different sources</li>
                        <li>Handle schema changes gracefully</li>
                        <li>Provide data quality monitoring</li>
                        <li>Scale horizontally as data volume grows</li>
                        <li>Recover from failures automatically</li>
                    </ul>

                    <h3>Solution Architecture</h3>
                    <p>
                        I designed a microservices-based ETL pipeline using Apache Airflow as the orchestration engine. 
                        The solution includes:
                    </p>
                    <ul>
                        <li><strong>Data Extraction:</strong> Parallel extraction from REST APIs, databases, and file systems</li>
                        <li><strong>Data Transformation:</strong> Schema validation, data cleaning, and business rule application</li>
                        <li><strong>Data Loading:</strong> Optimized bulk loading with conflict resolution</li>
                        <li><strong>Monitoring:</strong> Real-time alerts and comprehensive logging</li>
                        <li><strong>Recovery:</strong> Automatic retry mechanisms and failure handling</li>
                    </ul>
                </div>
                
                <div class="project-sidebar">
                    <div class="project-stats">
                        <h3>Key Metrics</h3>
                        <div class="stat-grid">
                            <div class="stat-item">
                                <span class="stat-number">1M+</span>
                                <span class="stat-label">Records/Day</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-number">60%</span>
                                <span class="stat-label">Faster Processing</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-number">99.9%</span>
                                <span class="stat-label">Data Quality</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-number">15+</span>
                                <span class="stat-label">Data Sources</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="project-timeline">
                        <h3>Development Timeline</h3>
                        <div class="timeline-item">
                            <div class="timeline-date">Week 1-2</div>
                            <div class="timeline-content">Requirements analysis and architecture design</div>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-date">Week 3-4</div>
                            <div class="timeline-content">Core ETL pipeline development</div>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-date">Week 5-6</div>
                            <div class="timeline-content">Data quality and monitoring implementation</div>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-date">Week 7-8</div>
                            <div class="timeline-content">Testing, optimization, and deployment</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Architecture Diagram -->
    <section class="project-section bg-gray">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>
            <div class="architecture-diagram">
                <div class="arch-layer">
                    <h3>Data Sources</h3>
                    <div class="arch-components">
                        <div class="arch-component">
                            <i class="fas fa-database"></i>
                            <span>PostgreSQL</span>
                        </div>
                        <div class="arch-component">
                            <i class="fas fa-cloud"></i>
                            <span>REST APIs</span>
                        </div>
                        <div class="arch-component">
                            <i class="fas fa-file-csv"></i>
                            <span>CSV Files</span>
                        </div>
                        <div class="arch-component">
                            <i class="fab fa-aws"></i>
                            <span>S3 Buckets</span>
                        </div>
                    </div>
                </div>
                
                <div class="arch-arrow">
                    <i class="fas fa-arrow-down"></i>
                </div>
                
                <div class="arch-layer">
                    <h3>ETL Processing</h3>
                    <div class="arch-components">
                        <div class="arch-component primary">
                            <i class="fas fa-cogs"></i>
                            <span>Apache Airflow</span>
                        </div>
                        <div class="arch-component">
                            <i class="fas fa-memory"></i>
                            <span>Redis Cache</span>
                        </div>
                        <div class="arch-component">
                            <i class="fab fa-docker"></i>
                            <span>Docker Containers</span>
                        </div>
                    </div>
                </div>
                
                <div class="arch-arrow">
                    <i class="fas fa-arrow-down"></i>
                </div>
                
                <div class="arch-layer">
                    <h3>Data Warehouse</h3>
                    <div class="arch-components">
                        <div class="arch-component">
                            <i class="fas fa-warehouse"></i>
                            <span>PostgreSQL DW</span>
                        </div>
                        <div class="arch-component">
                            <i class="fas fa-chart-bar"></i>
                            <span>Analytics Views</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Technical Implementation -->
    <section class="project-section">
        <div class="container">
            <h2 class="section-title">Technical Implementation</h2>
            
            <div class="implementation-tabs">
                <div class="tab-buttons">
                    <button class="tab-btn active" data-tab="extraction">Data Extraction</button>
                    <button class="tab-btn" data-tab="transformation">Transformation</button>
                    <button class="tab-btn" data-tab="loading">Data Loading</button>
                    <button class="tab-btn" data-tab="monitoring">Monitoring</button>
                </div>
                
                <div class="tab-content">
                    <div class="tab-panel active" id="extraction">
                        <h3>Parallel Data Extraction</h3>
                        <p>
                            The extraction layer uses Python's asyncio for concurrent API calls and connection pooling 
                            for database sources. Each source has its own extraction task with configurable retry logic.
                        </p>
                        
                        <pre><code class="language-python">
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import asyncio
import aiohttp

async def extract_api_data(source_config):
    """Extract data from API source with retry logic"""
    async with aiohttp.ClientSession() as session:
        for attempt in range(3):
            try:
                async with session.get(
                    source_config['url'],
                    headers=source_config['headers'],
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return validate_schema(data, source_config['schema'])
                    else:
                        raise Exception(f"HTTP {response.status}")
            except Exception as e:
                if attempt == 2:  # Last attempt
                    raise e
                await asyncio.sleep(2 ** attempt)  # Exponential backoff

def extract_all_sources(**context):
    """Orchestrate parallel extraction from all sources"""
    sources = get_source_configurations()
    loop = asyncio.get_event_loop()
    
    tasks = [extract_api_data(source) for source in sources]
    results = loop.run_until_complete(asyncio.gather(*tasks))
    
    # Store results in XCom for downstream tasks
    return {'extracted_data': results, 'timestamp': datetime.now()}
                        </code></pre>
                    </div>
                    
                    <div class="tab-panel" id="transformation">
                        <h3>Data Transformation Pipeline</h3>
                        <p>
                            Transformation includes schema validation, data type conversion, business rule application, 
                            and data quality checks. Each transformation is logged for audit purposes.
                        </p>
                        
                        <pre><code class="language-python">
import pandas as pd
from pydantic import BaseModel, ValidationError
from typing import List, Dict, Any

class DataQualityRule:
    """Data quality validation rules"""
    
    @staticmethod
    def check_completeness(df: pd.DataFrame, required_columns: List[str]) -> Dict:
        """Check for missing required data"""
        missing_data = {}
        for col in required_columns:
            if col in df.columns:
                missing_count = df[col].isnull().sum()
                missing_data[col] = {
                    'missing_count': missing_count,
                    'missing_percentage': (missing_count / len(df)) * 100
                }
        return missing_data
    
    @staticmethod
    def check_data_types(df: pd.DataFrame, schema: Dict) -> List[str]:
        """Validate data types match expected schema"""
        errors = []
        for col, expected_type in schema.items():
            if col in df.columns:
                if not df[col].dtype == expected_type:
                    errors.append(f"Column {col}: expected {expected_type}, got {df[col].dtype}")
        return errors

def transform_customer_data(raw_data: Dict) -> pd.DataFrame:
    """Transform customer data with business rules"""
    df = pd.DataFrame(raw_data)
    
    # Data cleaning
    df['email'] = df['email'].str.lower().str.strip()
    df['phone'] = df['phone'].str.replace(r'\D', '', regex=True)
    
    # Business rules
    df['customer_segment'] = df.apply(classify_customer_segment, axis=1)
    df['lifetime_value'] = calculate_lifetime_value(df)
    
    # Data quality checks
    quality_report = DataQualityRule.check_completeness(
        df, ['customer_id', 'email', 'registration_date']
    )
    
    if any(report['missing_percentage'] > 5 for report in quality_report.values()):
        raise ValueError("Data quality threshold exceeded")
    
    return df
                        </code></pre>
                    </div>
                    
                    <div class="tab-panel" id="loading">
                        <h3>Optimized Data Loading</h3>
                        <p>
                            The loading process uses bulk operations with conflict resolution. Data is loaded in batches 
                            with transaction management to ensure consistency.
                        </p>
                        
                        <pre><code class="language-python">
import psycopg2
from psycopg2.extras import execute_batch
import logging

class DataWarehouseLoader:
    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.logger = logging.getLogger(__name__)
    
    def bulk_upsert(self, table: str, data: pd.DataFrame, 
                   conflict_columns: List[str]) -> Dict:
        """Perform bulk upsert with conflict resolution"""
        
        cursor = self.conn.cursor()
        
        try:
            # Create temporary table
            temp_table = f"temp_{table}_{int(time.time())}"
            cursor.execute(f"""
                CREATE TEMP TABLE {temp_table} 
                (LIKE {table} INCLUDING DEFAULTS)
            """)
            
            # Bulk insert into temp table
            insert_query = f"""
                INSERT INTO {temp_table} ({', '.join(data.columns)})
                VALUES ({', '.join(['%s'] * len(data.columns))})
            """
            
            execute_batch(
                cursor, insert_query, 
                data.values.tolist(), 
                page_size=1000
            )
            
            # Upsert from temp table to main table
            conflict_clause = f"({', '.join(conflict_columns)})"
            update_clause = ', '.join([
                f"{col} = EXCLUDED.{col}" 
                for col in data.columns 
                if col not in conflict_columns
            ])
            
            upsert_query = f"""
                INSERT INTO {table} 
                SELECT * FROM {temp_table}
                ON CONFLICT {conflict_clause}
                DO UPDATE SET {update_clause}
            """
            
            cursor.execute(upsert_query)
            
            # Get statistics
            cursor.execute(f"SELECT COUNT(*) FROM {temp_table}")
            processed_count = cursor.fetchone()[0]
            
            self.conn.commit()
            
            return {
                'processed_records': processed_count,
                'status': 'success',
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            self.conn.rollback()
            self.logger.error(f"Load failed: {str(e)}")
            raise
        finally:
            cursor.close()
                        </code></pre>
                    </div>
                    
                    <div class="tab-panel" id="monitoring">
                        <h3>Monitoring & Alerting</h3>
                        <p>
                            Comprehensive monitoring includes pipeline health, data quality metrics, and performance tracking 
                            with automated alerting via Slack and email.
                        </p>
                        
                        <pre><code class="language-python">
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.models import Variable
import json

class PipelineMonitor:
    def __init__(self):
        self.slack_webhook = Variable.get("slack_webhook_url")
        
    def send_success_notification(self, context):
        """Send success notification with metrics"""
        task_instance = context['task_instance']
        dag_run = context['dag_run']
        
        metrics = task_instance.xcom_pull(key='pipeline_metrics')
        
        message = f"""
        ‚úÖ ETL Pipeline Completed Successfully
        
        üìä Metrics:
        ‚Ä¢ Records Processed: {metrics['total_records']:,}
        ‚Ä¢ Processing Time: {metrics['duration']} minutes
        ‚Ä¢ Data Quality Score: {metrics['quality_score']}%
        ‚Ä¢ Sources Processed: {metrics['sources_count']}
        
        üïê Execution Time: {dag_run.start_date}
        """
        
        return SlackWebhookOperator(
            task_id='success_notification',
            http_conn_id='slack_webhook',
            message=message,
            channel='#data-engineering'
        )
    
    def send_failure_alert(self, context):
        """Send failure alert with error details"""
        task_instance = context['task_instance']
        exception = context.get('exception')
        
        message = f"""
        üö® ETL Pipeline Failed
        
        ‚ùå Task: {task_instance.task_id}
        üìù Error: {str(exception)}
        üïê Failed At: {task_instance.start_date}
        
        Please check the logs for more details.
        """
        
        return SlackWebhookOperator(
            task_id='failure_alert',
            http_conn_id='slack_webhook',
            message=message,
            channel='#data-engineering-alerts'
        )

# DAG configuration with monitoring
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'on_success_callback': PipelineMonitor().send_success_notification,
    'on_failure_callback': PipelineMonitor().send_failure_alert
}
                        </code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results & Impact -->
    <section class="project-section bg-gray">
        <div class="container">
            <h2 class="section-title">Results & Business Impact</h2>
            
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-icon">
                        <i class="fas fa-tachometer-alt"></i>
                    </div>
                    <h3>Performance Improvements</h3>
                    <ul>
                        <li>Reduced processing time from 4 hours to 90 minutes (60% improvement)</li>
                        <li>Increased throughput to 1M+ records per day</li>
                        <li>Achieved 99.9% pipeline reliability</li>
                        <li>Reduced manual intervention by 95%</li>
                    </ul>
                </div>
                
                <div class="result-card">
                    <div class="result-icon">
                        <i class="fas fa-shield-alt"></i>
                    </div>
                    <h3>Data Quality</h3>
                    <ul>
                        <li>Implemented automated data validation rules</li>
                        <li>Reduced data errors by 85%</li>
                        <li>Added comprehensive audit logging</li>
                        <li>Real-time data quality monitoring</li>
                    </ul>
                </div>
                
                <div class="result-card">
                    <div class="result-icon">
                        <i class="fas fa-dollar-sign"></i>
                    </div>
                    <h3>Cost Savings</h3>
                    <ul>
                        <li>Eliminated 20 hours/week of manual work</li>
                        <li>Reduced infrastructure costs by 30%</li>
                        <li>Faster time-to-insight for business teams</li>
                        <li>Improved decision-making with reliable data</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Challenges & Solutions -->
    <section class="project-section">
        <div class="container">
            <h2 class="section-title">Challenges & Solutions</h2>
            
            <div class="challenges-list">
                <div class="challenge-item">
                    <div class="challenge-problem">
                        <h3><i class="fas fa-exclamation-triangle"></i> Schema Evolution</h3>
                        <p>
                            Data sources frequently changed their schema without notice, causing pipeline failures 
                            and data inconsistencies.
                        </p>
                    </div>
                    <div class="challenge-solution">
                        <h4>Solution</h4>
                        <p>
                            Implemented schema versioning with backward compatibility checks. Added automated schema 
                            detection and graceful degradation for missing fields. Created alerts for schema changes 
                            to proactively update transformation logic.
                        </p>
                    </div>
                </div>
                
                <div class="challenge-item">
                    <div class="challenge-problem">
                        <h3><i class="fas fa-clock"></i> Rate Limiting</h3>
                        <p>
                            External APIs had strict rate limits that caused extraction tasks to fail during 
                            peak processing times.
                        </p>
                    </div>
                    <div class="challenge-solution">
                        <h4>Solution</h4>
                        <p>
                            Implemented intelligent rate limiting with exponential backoff and jitter. Added request 
                            queuing with priority scheduling. Created fallback mechanisms to use cached data when 
                            rate limits are exceeded.
                        </p>
                    </div>
                </div>
                
                <div class="challenge-item">
                    <div class="challenge-problem">
                        <h3><i class="fas fa-database"></i> Data Volume Growth</h3>
                        <p>
                            As the business grew, data volume increased 10x, causing memory issues and 
                            processing timeouts.
                        </p>
                    </div>
                    <div class="challenge-solution">
                        <h4>Solution</h4>
                        <p>
                            Redesigned the pipeline to use streaming processing with Apache Kafka. Implemented 
                            data partitioning and parallel processing. Added horizontal scaling capabilities 
                            with Kubernetes for dynamic resource allocation.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Future Improvements -->
    <section class="project-section bg-gray">
        <div class="container">
            <h2 class="section-title">Future Improvements</h2>
            
            <div class="improvements-grid">
                <div class="improvement-card">
                    <h3><i class="fas fa-brain"></i> Machine Learning Integration</h3>
                    <p>
                        Add ML-based anomaly detection for data quality monitoring and predictive 
                        failure prevention.
                    </p>
                </div>
                
                <div class="improvement-card">
                    <h3><i class="fas fa-stream"></i> Real-time Processing</h3>
                    <p>
                        Migrate to a hybrid batch/streaming architecture using Apache Kafka and 
                        Spark Streaming for near real-time data processing.
                    </p>
                </div>
                
                <div class="improvement-card">
                    <h3><i class="fas fa-cloud"></i> Cloud Migration</h3>
                    <p>
                        Move to cloud-native services like AWS Glue, Lambda, and Kinesis for 
                        better scalability and reduced operational overhead.
                    </p>
                </div>
                
                <div class="improvement-card">
                    <h3><i class="fas fa-chart-line"></i> Advanced Analytics</h3>
                    <p>
                        Implement data lineage tracking and impact analysis to better understand 
                        data dependencies and downstream effects.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Project Navigation -->
    <section class="project-navigation">
        <div class="container">
            <div class="nav-links">
                <a href="../index.html#projects" class="nav-btn">
                    <i class="fas fa-arrow-left"></i>
                    Back to Projects
                </a>
                <a href="streaming-pipeline.html" class="nav-btn">
                    Next Project
                    <i class="fas fa-arrow-right"></i>
                </a>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-text">
                    <p>&copy; 2024 Omar Abdullatif. Built with passion for data engineering.</p>
                </div>
                <div class="footer-social">
                    <a href="https://github.com/omarabdullatiff" class="social-link">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/omar-abdullatif-799b53256" class="social-link">
                        <i class="fab fa-linkedin"></i>
                    </a>
                    <a href="mailto:omarabdullatiff000@gmail.com" class="social-link">
                        <i class="fas fa-envelope"></i>
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../js/script.js"></script>
    <script>
        // Tab functionality for technical implementation
        document.addEventListener('DOMContentLoaded', function() {
            const tabButtons = document.querySelectorAll('.tab-btn');
            const tabPanels = document.querySelectorAll('.tab-panel');
            
            tabButtons.forEach(button => {
                button.addEventListener('click', () => {
                    const targetTab = button.getAttribute('data-tab');
                    
                    // Remove active class from all buttons and panels
                    tabButtons.forEach(btn => btn.classList.remove('active'));
                    tabPanels.forEach(panel => panel.classList.remove('active'));
                    
                    // Add active class to clicked button and corresponding panel
                    button.classList.add('active');
                    document.getElementById(targetTab).classList.add('active');
                });
            });
        });
    </script>
</body>
</html>
